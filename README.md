# CS50AI | Lecture 4 - Learning | Project 4 - [Nim](https://cs50.harvard.edu/ai/2024/projects/4/nim/)

This project is a mandatory assignment from **CS50AI ‚Äì Lecture 4: "Learning"**, focusing on **Reinforcement Learning** using Q-learning. The objective is to train an AI agent to play the game of _Nim_ optimally through self-play, updating its strategy over time based on rewards and estimated future outcomes.

---

## üìå Usage

To run the project locally, follow these steps:

1. **Clone the Repository**  
   ```bash
   git clone https://github.com/jimmygian/cs50ai_nim.git
   cd nim-ai
   ```

2. **Modify play.py (Optional)**  
   You can train the AI by having it play against itself as many times as you want.  `human_player` can be set to 0 or 1 to specify whether human player moves first or second.
   ```python
   ai = train(10000, human_player=<0,1>)
   play(ai)
   ```

3. **Run `python play.py` in terminal to play Against the AI**  
   After training, you will be prompted to play against the trained agent:
   ```bash
    ...
    Playing training game 9999
    Playing training game 10000
    Done training

    Piles:
    Pile 0: 1
    Pile 1: 3
    Pile 2: 5
    Pile 3: 7

    Your Turn
    Choose Pile:
   ```

<br>

## Project Overview

This project involves implementing a Q-learning agent to play Nim, a mathematical game of strategy. The agent learns through repeated games against itself, using Q-values to assess the value of state‚Äìaction pairs. The learning process improves its decision-making over time.

The core concepts include:

- **Q-Learning**: Updating estimates for how valuable specific actions are in given game states.
- **Exploration vs Exploitation**: Balancing between trying new actions and exploiting known good ones.
- **State Representation**: Modeling the state of the game as a tuple of pile sizes.

---

## My Task

- **Implemented Key Q-learning Functions** inside the `NimAI` class:
  - `get_q_value(state, action)`
  - `update_q_value(state, action, old_q, reward, future_rewards)`
  - `best_future_reward(state)`
  - `choose_action(state, epsilon=True)`
- Focused on **reinforcement learning mechanics** and **epsilon-greedy exploration**.
- Avoided hardcoding Q-values ‚Äî the agent learns from scratch through trial and error.



## üí° Implementation Highlights

### `get_q_value(state, action)`
Returns the Q-value for a given state and action. If no Q-value is recorded yet, it defaults to 0.

### `update_q_value(state, action, old_q, reward, future_rewards)`
Applies the Q-learning update rule:

```python
Q(s, a) ‚Üê old_q + Œ± * (reward + Œ≥ * future_rewards ‚àí old_q)
```

- `Œ±`: Learning rate (how quickly the model updates)
- `Œ≥`: Discount factor (future vs. immediate reward; default is 1)

### `best_future_reward(state)`
Looks at all possible actions from the given state and returns the highest Q-value seen so far. If no Q-values exist for the actions, returns 0.

### `choose_action(state, epsilon=True)`
Selects an action using an **epsilon-greedy** strategy:
- With probability `epsilon`, picks a **random** action (exploration).
- Otherwise, picks the **best known** action (exploitation).

---

## Training Logic

The `train(n)` function plays `n` games where the AI plays against itself. After each move, it updates its knowledge based on:

- The old state and action.
- The resulting new state.
- Whether the move led to a win/loss (reward: +1 / -1).
- If the game continues, intermediate rewards are 0.

This reinforces winning strategies and penalizes losing ones over many iterations.

---

## üîó References

- [CS50AI Nim Project Page](https://cs50.harvard.edu/ai/2024/projects/4/nim/)
- [Q-Learning](https://en.wikipedia.org/wiki/Q-learning)
- [Epsilon-Greedy Strategy](https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/)

---

## üöÄ Conclusion

This project demonstrates how an agent can learn to play a game from scratch using **reinforcement learning** techniques. By evaluating rewards and refining its Q-values over time, the agent can evolve into a strong player ‚Äî all without ever being explicitly told the rules or strategy.
